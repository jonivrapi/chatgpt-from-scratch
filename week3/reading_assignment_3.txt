watch: https://www.youtube.com/watch?v=gQddtTdmG_8
read: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/

Q1. In the readings we see an old version of a positional encoding (sine function) and an even older version of word embeddings (word2vec). What are the pros and cons of learning these embeddings directly as part of our larger model, as opposed to a pre-defined/pre-learned embedding as described in the readings?

My hunch here is that, much like fine-tuning a pre-trained model for some other task can lead to a good model under certain circumstances (for example if you are fine-tuning a pre-trained imagenet model on some more specific task with very limited data), using pre-learned embeddings like word2vec can lead to a good model if the domain you are training a model on is severely limited in data and the word2vec embeddings were learned over a relatively similar domain. That being said, if you have the data, learning the embeddings as part of the larger model will likely lead to better performance, as the embeddings will be more specific to the task at hand. An example here may be that you have a pre-learned word2vec embedding trained on news articles like was mentioned in the video, but you are training a model to do completions over a medical corpus. It is likely true that there is not a massive amount of overlap between the two domains, and as such the word2vec embeddings will not be very useful to the model you are training, and may even actively hurt it.